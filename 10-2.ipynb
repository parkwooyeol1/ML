{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c5e4943b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/parkwooyeol/workspace/myenv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-09-03 20:03:59.340178: I tensorflow/core/util/port.cc:111] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-09-03 20:03:59.472855: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-09-03 20:03:59.472912: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-09-03 20:03:59.473616: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-09-03 20:03:59.534761: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-09-03 20:04:00.333446: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "# BART 모델 -> 트랜스포머 기반 인코더-디코더 모델\n",
    "\n",
    "# 허깅스페이스로 distilbart 모델 로드하기 -> CNN 텍스트 데이터셋에서 미세 튜닝된 모델, 텍스트를 56~142자 사이의 길이로 요약, min_length, max_length 매개변수로 변경가능\n",
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(task='summarization', model='sshleifer/distilbart-cnn-12-6', device=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dbc2f2d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'summary_text': \" Vincent Willem van Gogh was a Dutch Post-Impressionist painter . His oeuvre includes landscapes, still lifes, portraits and self-portraits . Only one of Van Gogh's paintings, The Red Vineyard, was sold during his lifetime . He died from a self-inflicted gunshot at age 37 .\"}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pipe 객체로 텍스트 요약\n",
    "sample_text = \"\"\"Vincent Willem van Gogh was a Dutch Post-Impressionist \n",
    "painter who is among the most famous and influential figures in the history of Western art.\n",
    "In just over a decade, he created approximately 2,100 artworks, including around 860 oil paintings, \n",
    "most of them in the last two years of his life. His oeuvre includes landscapes, still lifes, portraits, \n",
    "and self-portraits, most of which are characterised by bold colours and dramatic brushwork that contributed\n",
    "to the rise of expressionism in modern art. Van Gogh's work was only beginning to gain critical attention before \n",
    "he died from a self-inflicted gunshot at age 37.[5] During his lifetime, only one of Van Gogh's paintings, The Red Vineyard, was sold.\n",
    "\"\"\"\n",
    "\n",
    "pipe(sample_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bee81722",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You passed `num_labels=3` which is incompatible to the `id2label` map of length `2`.\n",
      "You passed `num_labels=3` which is incompatible to the `id2label` map of length `2`.\n",
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "# KoBART 모델 -> SKT에서 만든 BART기반의 한국어 인코더-디코더 모델\n",
    "kobart = pipeline(task='summarization', model='EbanLee/kobart-summary-v3', device=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e3fde127",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'summary_text': '바이오 분야에서 급성장 추세인 중국이 세계 3대 혁신 신약 시장으로 올라설 것이라는 조사 결과가 나왔는데 중국이 혁신 시장 규모 세계 3위인 독일을 조만간 제칠 것으로 분석됐다.'}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ko_text = \"\"\"\n",
    "신약 후보 물질 발굴을 비롯해 바이오 분야에서 급성장 추세인 중국이 세계 3대 혁신 신약 시장으로 올라설 것이라는 조사 결과가 나왔다. \n",
    "맥킨지앤드컴퍼니가 오는 9월 말에 공개할 ‘아시아 바이오파마 보고서’에 따르면, 중국이 혁신 시장 규모 세계 3위인 독일을 조만간 제칠 것으로 분석됐다.\n",
    "중국은 세계 2위 제약 시장이지만, 복제약(제네릭)이 대부분을 차지해 혁신 신약에선 약한 국가로 여겨졌다. 하지만 최근 들어 신약 비중이 가파르게 커지는 추세다. \n",
    "현재까지 중국이 내놓은 혁신 신약 중 매출 1억달러(약 1400억원)를 넘긴 의약품은 15종으로, 이 가운데는 블록버스터(연간 매출 1조원 이상인 의약품)가 된 신약도 있다.\n",
    "중국 바이오 시장이 양적 성장을 넘어 질적으로도 눈에 띄게 도약한다는 분석이 나오는 배경이다.\n",
    "맥킨지 보고서에 따르면, 2023년 중국 의약품 시장은 1250억달러(약 174조원) 규모다. 이 중에서도 혁신 신약 부문은 연평균 16% 성장하면서 시장을 견인해왔다. \n",
    "특히 중국은 자국 기업을 중심으로 빠르게 성장하고 있다. 맥킨지는 2023년 기준으로 중국 시장의 다국적 제약사 점유율은 75%지만, 2028년에는 60%로 줄어들 것으로 전망했다.\n",
    "맥킨지는 또한 중국의 혁신 신약 시장이 지금까지는 미국, 일본, 독일에 밀렸으나, 2028년엔 독일을 제치고 3위로 오를 것이라고 내다봤다.\n",
    "중국의 혁신 신약 시장이 이처럼 빠르게 성장하는 배경으로 12곳이 넘는 바이오 혁신 허브가 꼽힌다. 바이오 혁신 허브를 중심으로 빠르게 기술력을 키워가고 있다는 것이다. \n",
    "중국 정부의 적극적인 R&D 지원과 규제 혁신도 큰 도움을 줬다. 규제를 크게 낮춘 덕분에 신약 보험 등재 기간이 단축됐고, 이를 통해 시장이 빠르게 확대됐다는 것이다. \n",
    "중국의 신약 후보 물질 기술 수출 규모는 계속 늘고 있다. 2019년 10억달러에서 2024년 575억달러로 늘어났다. 2019~2020년까지만 해도 세계 신약 후보 물질 기술 수출 중 중국이 차지하는 \n",
    "비율은 4%에 그쳤으나, 2025년엔 40% 가까이 될 것으로 전망된다. 의약품 위탁 생산 기술이 빠르게 발달하는 것도 중국의 혁신 신약 시장 성장에 영향을 미치고 있다. \n",
    "현재 글로벌 빅파마 10곳 중 3곳은 중국에서 약을 위탁 생산하고 있다.\n",
    "중국에서 나온 혁신 신약 중 15종은 이미 매출 1억달러를 넘겼다. 이중에서도 특히 중국 제약회사 베이진(BeiGene)이 \n",
    "개발한 혈액암 치료제(상품명 브루킨사)는 이미 2023년에 연매출 13억달러(1조8189억원)을 기록했다. 지난해엔 20억달러(2조8000억원)로 매출 2조원을 넘어섰다.\n",
    "글로벌 신약의 경우엔 보통 연간 매출액 1조원이 넘으면 ‘블록버스터’라고 칭한다. 맥킨지앤드컴퍼니의 박준형 파트너는브루킨사는 중국에서 개발된 최초의 블록버스터이자 \n",
    "중국산 의약품 최초로 FDA(미 식품의약국) 승인을 획득한 항암제로, 중국의 신약 기술력이 상당히 무르익었음을 보여주는 사례라고 했다.\n",
    "중국 바이오사 레전드바이오텍이 개발하고 얀센에 수출한 다발골수치료제 ‘카빅티’ 역시 지난해 약 9억6000만달러(1조3360억원) 매출을 올렸다. \n",
    "중국이 앞으로 줄줄이 블록버스터를 쏟아낼 가능성이 높다는 얘기가 나오는 이유다.\n",
    "맥킨지 보고서에 따르면, 중국은 전임상 후보 물질 도출 속도가 글로벌 평균보다 2배 이상 빠른 나라이기도 하다. 신약 개발 과정에서 후보 물질을 찾는 단계가 다른 나라보다 훨씬 빠르게 진행된다는 뜻이다.\n",
    "박준형 파트너는글로벌 제약사는 타깃을 발굴하고 후보 물질을 확정하기까지 4~6년이 걸리지만, 최근 중국 바이오텍들은 이 같은 과정을 2~3년 안에 끝낼 수 있다면서 AI(인공지능) 기반으로 약물을 발굴하고,\n",
    "수천 명의 인력과 엄청난 자본을 한꺼번에 투입한 결과라고 했다. 인구가 많다 보니 임상 환자 풀이 커서 신약 임상 추진 속도를 높일 수 있는 것도 장점으로 꼽힌다.\n",
    "바이오 업계 관계자는 중국은 바이오 기초 연구도 탄탄해 셀, 네이처, 사이언스 같은 대표적인 과학 저널에 발표된 의학 부문 논문이 급증하는 추세”라며 \n",
    "제약 바이오 기술력에서 중국에 뒤처지지 않기 위해 한국 바이오 업계와 정부가 대책을 세워야 한다”고 했다.\n",
    "\"\"\"\n",
    "kobart(ko_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "37d0b14d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# kobart 모델은 빔서치라는 방법을 사용해 텍스트를 생성하기 때문에 실행할 때마다 결과가 다를 수 있음, 최대 300자까지 출력\n",
    "# 빔서치 -> 토큰 단계마다 가장 가능성이 높은 n개의 시퀀스를 유지하면서 다음 토큰을 생성하여 이어가는 방법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "565e91a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "30000"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 텍스트 토큰화\n",
    "# kobart -> 문자수준의 BPE알고리즘사용\n",
    "\n",
    "# kobart 모델의 어휘 사전 크기\n",
    "print(kobart.tokenizer.vocab_size)\n",
    "len(kobart.tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9049b75d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30000"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocap = kobart.tokenizer.vocab\n",
    "len(vocap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c9adb5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('▁꽤', 21156),\n",
       " ('▁AP', 23804),\n",
       " (\"▁'무\", 20842),\n",
       " ('洮', 5085),\n",
       " ('齋', 8957),\n",
       " ('▁내놓', 15951),\n",
       " ('▁밝혔습니다.\\n', 20146),\n",
       " ('彝', 3798),\n",
       " ('체인', 27245),\n",
       " ('▁우즈', 22840)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 토큰 출력하여 확인\n",
    "list(vocap.items())[:10] # (토큰, 토큰아이디), _문자는 공백 -> 해당토큰 앞에 공백이 있으므로 단어의 시작부분을 나타냄"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f6994af0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁혼자', '▁만들', '면서', '▁공부', '하는', '▁', '딥', '러', '닝']\n"
     ]
    }
   ],
   "source": [
    "# 샘플텍스트를 토큰으로 나눠보기\n",
    "token = kobart.tokenizer.tokenize('혼자 만들면서 공부하는 딥러닝')\n",
    "print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b03b8b5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[16814, 14397, 14125, 16962, 14049, 1700, 10021, 10277, 9747]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 각 토큰의 토큰아이디 확인\n",
    "kobart.tokenizer.convert_tokens_to_ids(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37cfd2fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 16814, 14397, 14125, 16962, 14049, 1700, 10021, 10277, 9747, 1]\n"
     ]
    }
   ],
   "source": [
    "# encode() 메서드는 문자열에서 토큰 아이디 리스트를 바로 만들어 줌\n",
    "token_ids = kobart.tokenizer.encode('혼자 만들면서 공부하는 딥러닝')\n",
    "print(token_ids) # 0,1은 문자열의 시작과 끝을 나타냄"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ed3d8c4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s>', '▁혼자', '▁만들', '면서', '▁공부', '하는', '▁', '딥', '러', '닝', '</s>']\n"
     ]
    }
   ],
   "source": [
    "# 토큰아이디를 토큰으로 만들기\n",
    "token = kobart.tokenizer.convert_ids_to_tokens(token_ids)\n",
    "print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4b513f15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s> 혼자 만들면서 공부하는 딥러닝</s>'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 원래 문자열로 복원\n",
    "kobart.tokenizer.decode(token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5829ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BPE는 가장 많이 등장하는 부분단어 선택 but 워드피스는 부분단어를 구성하는 개별 토큰의 빈도도 고려\n",
    "# 유니그램 -> 초기에 매우 큰 어휘 사전을 만든 다음 사전에 지정한 어휘 사전 크기에 도달할 때까지 점진적으로 토큰을 제거한 다음\n",
    "# 어휘사전에 있는 모든 토큰이 독립적이라 가정하고, 전체 손실을 가장 적게 증가시키는 토큰을 하나씩 삭제(음의 로그 가능도)\n",
    "# 센텐스피스 -> 사전 토큰화(공백 등을 기준으로 분할)를 하지 않는 방식"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
